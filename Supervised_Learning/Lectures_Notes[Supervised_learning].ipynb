{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lectures_Notes[Supervised_learning].ipynb",
      "provenance": [],
      "collapsed_sections": [
        "7GQKzbP7oSKm",
        "sbeTIYqw31wx"
      ],
      "authorship_tag": "ABX9TyMBqmMkRzTWdFuA3FfH0OSl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/platonovaleria/Coursera/blob/main/Lectures_Notes%5BSupervised_learning%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GQKzbP7oSKm"
      },
      "source": [
        "#Введение (Неделя 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbhFdwqb197t"
      },
      "source": [
        "##Задачи обучения с учителем:\n",
        "\n",
        "1. классификация (бинарная, многоклассовая)\n",
        "2. регрессия (ответ - все вещественные числа)\n",
        "3. ранжирование (поисковик)\n",
        "\n",
        "##Задачи обучения без учителя:\n",
        "\n",
        "1. Кластеризация (сегментация пользователей, поиск схожих групп)\n",
        "2. Визуализация (понижение размерности, поиск структуры)\n",
        "3. поиск аномалий\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbeTIYqw31wx"
      },
      "source": [
        "##Признаки в машинном обучении:\n",
        "\n",
        "* описывает объект\n",
        "\n",
        "1. бинарные (D_j = {0, 1})\n",
        "2. вещественные (возраст, площадь м2)\n",
        "3. категориальные (цвет глаз, город) - в отличие от вещественных нельзя сравнить <, >, только на равенство\n",
        "    \n",
        "    3.1 порядковые признаки (упорядоченные множества - тип населенного пункта, иерархические) - в отличие от вещественных нельзя произвести вычитания и сложения, определить величину разности\n",
        "\n",
        "4. множествозначные (какие слова входят в текст определенной тематики, все фильмы. просмотренные пользователем)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJJC5WQO5HYC"
      },
      "source": [
        "###Проблемы признаков:\n",
        "\n",
        "* выбросы - отличаются от большинства объектов\n",
        "* малые количественные значения по категориальным признакам\n",
        "* распределение с \"тяжелым хвостом\" - разбить или привести к нормальному виду"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbf6YiBPocqQ"
      },
      "source": [
        "#Линейные модели в задачах регрессии"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOJ7pCKk5v6Y"
      },
      "source": [
        "##Градиент\n",
        "\n",
        "Градиентные методы возможно использовать только при функционале ошибки, имеющем гладкую функцию (наличие поизводной в нуле, дифференциируемая функция на всем интервале значений)\n",
        "\n",
        "* сток в матрице по кол-ву объектов\n",
        "* столцов - по признакам\n",
        "\n",
        "\n",
        "* Из выпуклости ф-ции следует, что к нее только один минимум (градиент на самом деле антиградиент)\n",
        "* Из гладкости - дифференциируема в любой точке, значит градиент (производную) можно определить в любой точке\n",
        "\n",
        "**Градиент** - вектор частных производных функционала ошибки по всем его параметрам.\n",
        "\n",
        "\n",
        "Расхождение градиентного спуска может быть в частности из-за большой величины шага\n",
        "\n",
        "Переменный шаг - уменьшение к точке минимума = $\\nu_{t}$  = k/t,где k - подбираемая константа, t - итерация\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjhwVkNeD9hK"
      },
      "source": [
        "##Стохастический градиентный спуск SGD\n",
        "\n",
        "В классическом градиентном спуске на одном градиентном шаге производится суммирование функционала ошибки по всей обучающей выборке, он занимает много времени\n",
        "\n",
        "В стохастическом градиентном спуске на одной итерации вычисляется не вектор градиента по всей выборке, а на случайно выбранном объекте обучающей выборки\n",
        "\n",
        "###Преимущества:\n",
        "\n",
        "* быстрее выполняется один шаг\n",
        "* не требуется хранить всю выборку в памяти\n",
        "* обучение по одному объекту, если это обусловлено типом данных (если последовательно во времени добавляются новые значения в выборку - онлайн-обучение моделей, пользовательские предпочтения)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FFpITmJFmZ3"
      },
      "source": [
        "##Линейные классификаторы\n",
        "\n",
        "* на основе регрессии: вместо вещественного значения берется знак + / -\n",
        "* модуль значения может использоваться для определения качества классификации - чем дальше объект от разделяющей прямой, тем лучше классификатор\n",
        "\n",
        "**Отступ** - скалярное произведение вектора весов на объект, умноженное на истинный ответ на этом объекте (> 0 - корректный ответ)\n",
        "\n",
        "###Функции потерь:\n",
        "\n",
        "* доля непрвильных ответов: разрывная в нуле (пороговая), можно использовать только негладкую оптимизацию, но она сложна и не гарантирует схождение\n",
        "* логистическая: $L(M) = ln(1 + exp(M)$\n",
        "* экспоненциальная: $L(M)=exp(-M)$\n",
        "* кусочно-линейная: $L(M)=max(0,1 - M)$  (M - отступ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhrYinh9okC_"
      },
      "source": [
        "#Проблема переобучения (Неделя №2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTLtGc1voouD"
      },
      "source": [
        "Как выявить: большие веса\n",
        "\n",
        "Как избежать:\n",
        "\n",
        "* отложенная выборка для проверки (train, test)\n",
        "* кросс-валидация\n",
        "* меры сложности модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3DIY5kku1h9"
      },
      "source": [
        "##Регуляризация\n",
        "\n",
        "**Мультиколлинеарность** - линейная зависимость признаков, неустойчивость решения, по факту у коллинеарных признаков взаимоисключающие веса при параметрах:\n",
        "* бесконечно много оптимальных алгоритмов\n",
        "* многие из них имеют большие веса\n",
        "* не все их них хорошо воспроизводят данные на тесте\n",
        "\n",
        "**Коэффициент регуляризации $\\lambda$** похож по сути на шаг градиента. Чем больше $\\lambda$, тем ниэе сложность модели - большее ограничение значений, веса перестают быть \"гибкими\", т.к большую часть значения веса при коэффициенте берет на себя регуляризация.\n",
        "\n",
        "L2-регуляризатор гладкий и выпуклый (почитать дополнительно)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNsklmLu3DXf"
      },
      "source": [
        "##Оценивание качества алгоритмов\n",
        "\n",
        "###Отложенная выборка \n",
        "\n",
        "подходит когда очень много данных\n",
        "\n",
        "* преимущество: алгоритм обучается один раз\n",
        "* недостаток: качество зависит от пропорции разбиения и выбора объектов, которые попадут в тест и трейн (70/30, 80/20)\n",
        "\n",
        "**Повышение качества**: n тестовых выборок с усреднением оценки (*при этом нет гарантии, что каждый объект побывает в обучающей выборке*)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJvsLhxe44SW"
      },
      "source": [
        "###Кросс-валидация\n",
        "\n",
        "**Принцип:** Разбитие выборки на k блоков, каждый далее используется как тест с устреднением качества"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR6vC4gL3AKn"
      },
      "source": [
        "##Сравнение алгоритмов и выбор гиперпараметров\n",
        "\n",
        "* выбор функционала\n",
        "* выбор регуляризации\n",
        "* выбор типа алгоритма\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zas7EZogIJiS"
      },
      "source": [
        "##Метрики качества в задачах регрессии\n",
        "\n",
        "Используются в:\n",
        "\n",
        "* задании функционала ошибки (при обучении) - возможно использовать разные метрики в обучающей и тестовой выборке\n",
        "* подборе гиперпараметров (измерение качества на кросс-валидации)\n",
        "* итоговой оценке модели\n",
        "\n",
        "\n",
        "**Среднеквадратичная ошибка MSE:** легко минимизировать, но сильно штрафует за большие ошибки (возведение отклонения в квадрат), не подходит для задач с выбросами\n",
        "\n",
        "**Средняя абсолютная ошибка MAE:** сложнее минимизировать, т.к. производная у модуля отклонений у нуле отсутствует, но данная метрика устойчивее к выбросам\n",
        "\n",
        "**Коэффициент детерминации:** отображает долю дисперсии, объясненную моделью в общей дисперсии ответов, т.е. значение явно интерпретируется - R = 1 идеальная модель"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4sJet0AK-V4"
      },
      "source": [
        "##[!] Несимметричные потери\n",
        "\n",
        "* штраф за недопрогноз > штрафа за перепрогноз: квантильная ошибка - почитать дополнительно"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxPeXNfXRPot"
      },
      "source": [
        "##Метрики качества классификации\n",
        "\n",
        "**Accuracy** - доля правильных ответов. \n",
        "\n",
        "* Плохо работает на несбалансированных выборках. В этом случае есть смысл измерять долю правильных ответов на самом крупном классе.\n",
        "* Ошибки могут иметь разную цену (пример с невозвратом кредита)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcX9vIfUSyY5"
      },
      "source": [
        "##Точность и полнота\n",
        "\n",
        "**Матрица ошибок:**\n",
        "\n",
        "[ True positive ] [ False positive ]\n",
        "\n",
        "[ False Negative ] [ True Negative ]\n",
        "\n",
        "* TP - ans_alg = 1, y = 1 (срабатывание алг.)\n",
        "* FP - ans_alg = 1, y = -1 (ошибка срабатывания алг.)\n",
        "* FN - ans_alg = -1, y = 1 (ошибка пропуска алг.)\n",
        "* TN - ans_alg = -1, y = -1 (пропуск алг.)\n",
        "\n",
        "**Точность:** насколько мы можем доверять классификатору при ans_alg = 1\n",
        "\n",
        "*precision (a, X) = TP / (TP+FP)* - отношение верных срабатываний к общему числу срабатываний\n",
        "\n",
        "**Полнота:** как много положительных объектов находит классификатор\n",
        "\n",
        "*recall(a, X) = TP / (TP+FN)* - отношение верных срабатываний к общему числу  объектов 1 класса в выборке\n",
        "\n",
        "Классификаторы **высокой точности** могут срабатывать малое количество раз и не ошибаться, пропуская при этом много объектов, на которых тоже должны были сработать. **Полнота** как раз и отображает насколько \"часто\" алгоритм сработал.\n",
        "\n",
        " Данные метрики хорошо подходят для несбалансированных классов\n",
        "\n",
        "В прикладных хадачах обычно фиксируют минимальное значение одной метрики и оптимизируют другую при настройке и оценке качества алгоритма"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HN8UliuPQp5"
      },
      "source": [
        "###Объединение точности и полноты\n",
        "\n",
        "Для оптимизации обеих метрик можно объединить их в одну:\n",
        "\n",
        "* арифметическое среднее (пример проблемы: выборка с 10% объектов класса 1, константный алгоритм. выдающий ответ 1, pres = 0.1, recall = 1, A = 0.55, константные и разумные алгоритмы могут получить одинаковую оценку)\n",
        "* минимум (не учитывается величина второго параметра в сравнении алгоритмов)\n",
        "* гармоническое среднее: 2\\*pres*recall / (pres + recall) - сглаженный минимум\n",
        "\n",
        "В расширенной F-мере:\n",
        "* при $\\beta$ = 0.5 важнее окажется полнота\n",
        "* при $\\beta$ = 2 важнее точность"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLJsm31Gr4vr"
      },
      "source": [
        "#Линейные модели: статистический взгляд (Неделя №3)\n",
        "\n",
        "##Метод максимального правдоподобия\n",
        "\n",
        "***моделирование счетчика распределенеим пуассона - погуглилть дл каких физических явлений какие распределения подходят***\n",
        "\n",
        "Метод максимального правдоподобия\n",
        "\n",
        "Разобранный пример:\n",
        "\n",
        "Предположим, что данные, которые являются счетчиком, хорошо описываются распределением Пуассона, в котором фигурирует неизвестный параметр $\\lambda$, который нужно задать в нашу искомую зависимость (настраиваемый параметр распределения, как коэффициенты a и b в уравнении прямой).\n",
        "\n",
        "Далее рассмотрим функцию вероятности распределения для распределения Пуассона. По ней считают, с какой вероятностью случайная величина из распределения Пуассона с параметром λ примет значение k.\n",
        "\n",
        "Для выборок, состоящих из независимых величин с одинаковым распределением, мы можем просуммировать вероятности получения всех элементов, т.е. суммарную вероятность получения имено такой выборки по известным значениям элементов и их реализации."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMozguLfhdJR"
      },
      "source": [
        "##Переобучение регрессионных моделей\n",
        "\n",
        "Решения проблемы:\n",
        "* больше данных\n",
        "* меньше признаков\n",
        "* ограничение весов у признаков (большие по модулю - признак переобучения)\n",
        "\n",
        "L2 регуляризатор - гребневая регрессия\n",
        "L1 регуляризатор - лассо\n",
        "\n",
        "**Важно:** константное слагаемое не должно входить в регуляризаторы регрессии\n",
        "\n",
        "Гребневая регрессия аналитически представляет собой добавление к транспонированной матрице признаков диагональной матрицы с параметром $\\lambda$. Для Лассо существует только численное решение.\n",
        "\n",
        "**Резюме:** L1 и L2 регуляризация дает смещенные оценки коэффициентов, но уменьшает ошибку модели за счет уменьшения дисперсии. Зануление признаков в Лассо может использоваться для отбора признаков."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGCpvEn0mao7"
      },
      "source": [
        "##Логистическая регрессия\n",
        "\n",
        "Аналог бинарной классификации.\n",
        "\n",
        "Предсказывание меток - если отклик:\n",
        "* > 0.5 - 1 класс\n",
        "* < 0.5 - 0 класс\n",
        "\n",
        "Оценивание вероятности принадлежности классу. Принцип работы:\n",
        "\n",
        "- Нужно привести множество действительных чисел, которые могут получиться в ответе, к интервалу [0, 1] и построим линейную регрессию: E(y|x) = g^-1(<w,x>) Т.е. строит линейную регрессию не от условного матожидания, а от фунции g этого матожидания (g - обобщенная линейная модель GLM)\n",
        "\n",
        "\n",
        "В бинарной классификации за функцию g принимают функцию сигмоиды. В одномерном случае w0 сигмоиды определяет ее положение на числовой оси, а w1 - форму (аналог угла наклона в линейное регрессии). Чем больше w1,тем круче угол наклона в области ее середины.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiT7fRL287sF"
      },
      "source": [
        "##Масштабирование признаков\n",
        "\n",
        "Градиентные методы работают хорошо, если признаки имеют близкий друг к другу порядок, их линии уровня похожи на круги\n",
        "\n",
        "**Нормализация** - отношение разности каждого из признаков и среднего значения по всей выборке к стандартному отклонению по выборке\n",
        "\n",
        "**Масштабирование на отрезок [0,1]** - отношение разности каждого из признаков с минимальным значением по выборке и разности максимального и минимального значения"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "039wVELf-96S"
      },
      "source": [
        "##Спрямляющие пространства\n",
        "\n",
        "1. квадратичные признаки - добавление к признакам из квадратов и попарных сумм\n",
        "2. логарифмирование - подходит для неотрицательных значений признаков (можно брать модули признаков) в выборках с тяжелыми хвостами для приведения их к нормальному виду (берем натуральных логарифм от каждого признака с прибавлением к нему единицы - чтобы исключить логарифмирование от нуля)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8hk4uRGRHw3"
      },
      "source": [
        "##Категориальные признаки\n",
        "\n",
        "Бинарное кодирование - один категориальный признак заменяется на n позиций, где n - количество возможных значений признака (1 - соответствие значению, 0 - несоответствие) - если в тестовой выборке присутствует новое значение признака - зануляем все значения"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viZtXV6ASPPX"
      },
      "source": [
        "##Несбалансированная выборка\n",
        "\n",
        "Пример бинарной несбалансированной выборки - объектов класса 0 10% и менее (мед. диагностика, аномалии, классификация текстов)\n",
        "\n",
        "1. **Undersampling** - уменьшение бОльших классов (размер настраивается гиперпараметром)\n",
        "2. **Oversampling** - случайное лублирование элементов из меньших классов.\n",
        "\n",
        "В кросс-валидации необходимо использовать **стратифициованные** выборки, в которых гарантированно присутствуют объекты всех классов.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bHQT-rwTvIt"
      },
      "source": [
        "#Решающие деревья (Неделя №4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwgVqSNrWF9w"
      },
      "source": [
        ""
      ]
    }
  ]
}
